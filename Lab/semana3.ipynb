{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer, make_circles, make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import imdb, mnist, reuters\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXlQ9v7i_xBu"
   },
   "source": [
    "# El perceptrón\n",
    "\n",
    "El perceptrón es un sistema muy simple y podemos programarlo sin gran dificultad. Comenzaremos mostrando cómo un perceptrón va evolucionando hasta aprender a clasificar los datos. El perceptrón realiza la predicción y actualiza los pesos acorde a lo que se define a continuación:\n",
    "\n",
    "1. **Predicción:** el valor de la clase ($y$) se decide acorde a la siguiente fórmula.\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    1 & \\text{si } \\sum_{i=1}^{n} w_i x_i + b > 0 \\\\\n",
    "    0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $y$ es la salida (predicción) del perceptrón.\n",
    "- $w_i$ son los pesos asociados a cada entrada $x_i$.\n",
    "- $b$ es el sesgo (bias).\n",
    "- $n$ es el número de características de entrada.\n",
    "\n",
    "2. **Actualización de pesos:** Una vez predicha la clase se actualiza cada peso de la red ($w_i$) acorde a lo siguiente:\n",
    "$$ w_i = w_i + \\text{tasa de aprendizaje} \\times (y_{\\text{real}} - y_{\\text{predicha}}) \\times x_i $$\n",
    "\n",
    "Donde:\n",
    "- $w_i$ se actualiza para cada característica de entrada.\n",
    "- $y_{\\text{real}}$ es la etiqueta real del ejemplo de entrenamiento.\n",
    "- $y_{\\text{predicha}}$ es la predicción del perceptrón.\n",
    "\n",
    "El sesgo también se actualiza de manera similar:\n",
    "$$b = b + \\text{tasa de aprendizaje} \\times (y_{\\text{real}} - y_{\\text{predicha}})$$\n",
    "\n",
    "A continuación daremos una implementación del perceptrón donde:\n",
    "\n",
    "- `self.weights` representa $w_i$ (los pesos).\n",
    "- `self.weights[0]` representa el sesgo $b$.\n",
    "- `self.predict(inputs)` calcula $y_{\\text{predicha}}$.\n",
    "- `label` representa $y_{\\text{real}}$.\n",
    "- La actualización de pesos se realiza en la función `train` del perceptrón.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hp7gFHRADeA"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.01, max_epochs=100):\n",
    "        self.weights = np.random.rand(input_size + 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return 1 if summation > 0 else 0\n",
    "\n",
    "    def train(self, training_inputs, labels, visualize=False):\n",
    "        fig, axes = plt.subplots(1, self.max_epochs, figsize=(20, 6))  # Crear una fila de subfiguras\n",
    "        fig.suptitle('Perceptrón - Evolución por Época', y=1.02)\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "            # Verificar si se ha alcanzado la convergencia antes de mostrar la subfigura\n",
    "            if visualize and all(self.predict(inputs) == label for inputs, label in zip(training_inputs, labels)):\n",
    "                print(f'Convergencia alcanzada en la época {epoch + 1}')\n",
    "                break\n",
    "\n",
    "            if visualize:\n",
    "                # Visualizar la frontera de decisión después de cada época en una subfigura\n",
    "                self.plot_decision_boundary(training_inputs, labels, axes[epoch], epoch + 1, self.max_epochs)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self, inputs, labels, ax, epoch, max_epochs):\n",
    "        ax.clear()\n",
    "        ax.scatter(inputs[:, 0], inputs[:, 1], c=labels, cmap='viridis')\n",
    "        ax.set_xlabel('Característica 1')\n",
    "        ax.set_ylabel('Característica 2')\n",
    "\n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        Z = np.array([self.predict([x, y]) for x, y in np.c_[xx.ravel(), yy.ravel()]])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "        ax.set_title(f'Época {epoch}/{max_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F0uodYIAc95"
   },
   "source": [
    "Probaremos esta función para un conjunto de datos linealmente separables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "ICiz-4WZBC-n",
    "outputId": "43d7c77c-2e5c-4cad-ef4e-8ac8bf2ce6b5"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.rand(100, 2)\n",
    "labels = (2 * data[:, 0] + 3 * data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos según la etiqueta\n",
    "class_0 = data[labels == 0]\n",
    "class_1 = data[labels == 1]\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', label='Class 0')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], marker='o', label='Class 1')\n",
    "\n",
    "# Etiquetas y leyenda\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualization of Binary Classification Data')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnsv_1E0BQve"
   },
   "source": [
    "El objetivo sería que la red neuronal llegara a aprender la siguiente división:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "wxl2KDiUBUc1",
    "outputId": "8f6a6122-20c2-4e18-90f2-80265ccc4195"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.rand(100, 2)\n",
    "labels = (2 * data[:, 0] + 3 * data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos según la etiqueta\n",
    "class_0 = data[labels == 0]\n",
    "class_1 = data[labels == 1]\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', label='Class 0')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], marker='o', label='Class 1')\n",
    "\n",
    "# Dibujar la frontera de decisión (línea recta en este caso)\n",
    "x_decision_boundary = np.linspace(0, 1, 100)\n",
    "y_decision_boundary = (2 - 2 * x_decision_boundary) / 3\n",
    "plt.plot(x_decision_boundary, y_decision_boundary, color='red', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Etiquetas y leyenda\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualization of Binary Classification Data')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIIIbOyPBbjb"
   },
   "source": [
    "Veámos cómo se computa utlizando la clase perceptrón implementada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "rAIeyRKqBdCL",
    "outputId": "7e158803-007f-4c3c-9d7e-6db25a64c702"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el perceptrón con visualización\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc23A1U3Bt9L"
   },
   "source": [
    "Ahora probemos en un conjunto de datos diferente. Es importante observar que la inicialización aleatoria de las variables puede implicar que la red tarde más o menos tiempo en converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X10Isi5xBz_-"
   },
   "outputs": [],
   "source": [
    "# Generar un conjunto de datos más desafiante\n",
    "data, labels = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "komubWpEB2Fk"
   },
   "source": [
    "Veámos como, dependiendo de la inicialización aleatoria de los pesos, la red tarda más o menos en realizar el aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "eK5mav_rB4sp",
    "outputId": "520bb380-7e33-4624-bdb6-2182547044a3"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "FWzJIwgUB5W9",
    "outputId": "27cf1772-da35-426d-cdde-2e9aba9205fe"
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "8l0ukiKDB6dR",
    "outputId": "7343c9ac-ee65-4ac5-b75d-d4085e168cf3"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR1J7XmkCIkI"
   },
   "source": [
    "# Introducción a `keras`\n",
    "\n",
    "Replicar un perceptrón en Keras es bastante sencillo, ya que como hemos visto el perceptrón es una red neuronal con una sola capa. En el siguiente ejemplo utilizaremos `Sequential()` para crear un modelo en el que pueden añadirse capas seucnecialmente. Como el perceptrón solo tiene una capa, esta se añade utilizando `add(Dense(...))` con una sola unidad. En este caso utilizaremos una función de activación sigmoide, ya que estamos realizando una tarea de clasificación binaria.\n",
    "\n",
    "Luego, el modelo se compila con un optimizador (utilizaremos Gradiente Descendente Estocástico), una función de pérdida (en este caso entropía cruzada binaria, adecuada para clasificación binaria), y una métrica (accuracy en este caso). Finalmente, el modelo se entrena con los datos de entrada y etiquetas, y se evalúa su rendimiento en los mismos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "dAEykaK_DqNp",
    "outputId": "381e570a-53b8-49eb-c73e-4399a7ef8988"
   },
   "outputs": [],
   "source": [
    "# Datos de entrada\n",
    "np.random.seed(1)\n",
    "data = np.random.rand(100, 2)\n",
    "labels = (2 * data[:, 0] + 3 * data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos según la etiqueta\n",
    "class_0 = data[labels == 0]\n",
    "class_1 = data[labels == 1]\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', label='Class 0')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], marker='o', label='Class 1')\n",
    "\n",
    "# Etiquetas y leyenda\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualización de los datos para clasificación bina')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoSaK8khCOvW",
    "outputId": "88055b90-13c8-481f-a1bb-e2a05ae2cbda"
   },
   "outputs": [],
   "source": [
    "# Crear un modelo secuencial (perceptrón)\n",
    "perceptron_model = Sequential()\n",
    "\n",
    "# Añadir una capa densa (totalmente conectada) con una sola unidad y función de activación sigmoid\n",
    "perceptron_model.add(Dense(units=1, input_dim=2, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo con un optimizador (gradiente descendente estocástico), función de pérdida y métrica\n",
    "perceptron_model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "perceptron_model.fit(data, labels, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VfrxgVpDyCt"
   },
   "source": [
    "La salida que se ve durante el entrenamiento contiene la siguiente información:\n",
    "\n",
    "- Epoch x/epochs: Indica la época en la que estás. Durante cada época, el modelo pasa por todos los datos de entrenamiento una vez.\n",
    "- x/batches: Indica el número de lotes (batches) que llevas completado en cada época y se actualiza dinámicamente. Mediante\n",
    "[==============================], la barra de progreso visual indica cuánto del total de lotes se ha completado en la época actual.\n",
    "- xs xxms/step: Indica que, en promedio, cada paso (o lote) ha tomado xx milisegundos y que la época en su totalidad ha tomado x segundos.\n",
    "- loss: El valor de la función de pérdida en el conjunto de entrenamiento al final de esta época. Este valor mide cuánto se desvían las predicciones del modelo respecto a las etiquetas reales.\n",
    "- accuracy: La precisión del modelo en el conjunto de entrenamiento al final de esta época. Este valor indica la proporción de ejemplos de entrenamiento que se clasificaron correctamente.\n",
    "- val_loss: El valor de la función de pérdida en el conjunto de validación al final de esta época. Es similar a la pérdida en el conjunto de entrenamiento, pero se evalúa en un conjunto de datos no utilizado para el entrenamiento.\n",
    "- val_accuracy: La precisión en el conjunto de validación al final de esta época. Al igual que la precisión en el conjunto de entrenamiento, indica la proporción de ejemplos de validación que se clasificaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrSEs4E0CQ2u"
   },
   "source": [
    "La red neuronal tiene la forma que se muestra a continuación. Vemos que las capas contienen información del tipo `(None, 2)`, que representa el tamaño del lote (batch size) y el número de neuronas en esa capa. El `None` en la forma indica que la dimensión correspondiente puede tener cualquier tamaño. En el contexto del tamaño del lote (batch size), significa que el modelo puede aceptar datos de entrada con tamaños de lote variables. Por este motivo, durante el entrenamiento y la predicción, puedes alimentar al modelo lotes de diferentes tamaños, y la red se adaptará en consecuencia. El tamaño real del lote se determina durante la ejecución. El número 2 en (None, 2) representa la cantidad de neuronas en esa capa específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "_yYMNUSaCTcb",
    "outputId": "b5d371cb-c9b0-41db-c4b3-684359bfc9ab"
   },
   "outputs": [],
   "source": [
    "plot_model(perceptron_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDplmLzWC3cb",
    "outputId": "1b337450-2f35-4814-94cb-2bfe227b3de1"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo en datos de prueba\n",
    "accuracy = perceptron_model.evaluate(data, labels)[1]\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzhXXcBSDBWN"
   },
   "source": [
    "Normalmente, querrás evaluar el modelo en un conjunto de datos de prueba separado para obtener una medida más realista de su rendimiento en datos no vistos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "EebXcliODGrv",
    "outputId": "7eec81cd-5fa2-4816-a2bc-2b1b47cd8624"
   },
   "outputs": [],
   "source": [
    "# Datos de entrada\n",
    "np.random.seed(2)\n",
    "test_data = np.random.rand(100, 2)\n",
    "test_labels = (2 * test_data[:, 0] + 3 * test_data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos según la etiqueta\n",
    "class_0_train = data[labels == 0]\n",
    "class_1_train = data[labels == 1]\n",
    "\n",
    "class_0_test = test_data[test_labels == 0]\n",
    "class_1_test = test_data[test_labels == 1]\n",
    "\n",
    "# Configurar subfiguras\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Subfigura 1: Datos de entrenamiento\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(class_0_train[:, 0], class_0_train[:, 1], marker='o', label='Class 0 (Train)')\n",
    "plt.scatter(class_1_train[:, 0], class_1_train[:, 1], marker='o', label='Class 1 (Train)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "\n",
    "# Subfigura 2: Datos de prueba\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(class_0_test[:, 0], class_0_test[:, 1], marker='o', label='Class 0 (Test)')\n",
    "plt.scatter(class_1_test[:, 0], class_1_test[:, 1], marker='o', label='Class 1 (Test)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1yiORejBULd",
    "outputId": "801ca1bf-e512-44cb-a4fb-50fc27f29b3a"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de test\n",
    "accuracy = perceptron_model.evaluate(test_data, test_labels)[1]\n",
    "print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rofYRVX6Jaqe"
   },
   "source": [
    "Muchas veces estos modelos requerirán un tiempo de ejecución elevado y no podremos permitirnos entrenarlos dos veces, por lo que en estas situaciones es crucial persistir el modelo generado. Una de las formas en las que podemos hacer esto es generando un archivo `.keras` (también puede exportarse a otros formatos como por ejemplo (`.h5`) utilizando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj3lTKTNKQYc"
   },
   "outputs": [],
   "source": [
    "perceptron_model.save(\"modelo.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0E4EPxSKyOR"
   },
   "source": [
    "Estos archivos pueden cargarse luego ejecutando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sf1WvK2GK0LJ",
    "outputId": "2b6a4eea-4362-4b41-f824-603f55f196e4"
   },
   "outputs": [],
   "source": [
    "modelo_cargado = load_model(\"modelo.keras\")\n",
    "modelo_cargado.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7YR93EtE1De"
   },
   "source": [
    "# Desglosando Keras\n",
    "\n",
    "Keras es una biblioteca de alto nivel para construir y entrenar modelos de redes neuronales en Python. Está diseñada para ser fácil de usar, modular y extensible. Keras proporciona una interfaz de alto nivel para construir modelos neuronales, mientras que internamente utiliza otras bibliotecas como TensorFlow para realizar cálculos eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks6tPJNiFU1m"
   },
   "source": [
    "## Definición del modelo\n",
    "\n",
    "En Keras, los **modelos** se definen como una secuencia de capas. El modelo más común es el modelo secuencial, que se crea mediante la clase `Sequential`. Como su propio nombre indica, puedes agregar capas a este modelo de manera secuencial.\n",
    "\n",
    "```{python}\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Crear un modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Agregar capas al modelo\n",
    "model.add(Dense(units=64, activation='relu', input_dim=input_size))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "Las capas son los bloques de construcción fundamentales en un modelo de Keras. Puedes usar diferentes tipos de capas, como capas densas (totalmente conectadas), capas de convolución, capas recurrentes, etc.\n",
    "\n",
    "```{python}\n",
    "from keras.layers import Dense, Conv2D, LSTM\n",
    "```\n",
    "\n",
    "Las funciones de activación se aplican a la salida de una capa y le dan no linealidad al modelo. Algunas funciones de activación comunes son ReLU, Sigmoid y Tanh.\n",
    "\n",
    "```{python}\n",
    "from keras.activations import relu, sigmoid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1lRiEqoG4kx"
   },
   "source": [
    "### Funciones de activación\n",
    "\n",
    "La función de activación de una capa en una red neuronal desempeña un papel crucial en la capacidad del modelo para aprender y representar patrones en los datos. En concreto en la definición de la arquitectura del código anterior hemos utilizado dos capas para definir el modelo y cada una de ellas tiene una función de activación distinta.\n",
    "\n",
    "- En la primera capa `Dense` con activación `relu`: La función de activación Rectified Linear Unit es comúnmente utilizada en capas ocultas de una red neuronal ya que es simple pero efectiva. Matemáticamente, ReLU se define como $$f(x) = \\max(0, x)$$ lo cual significa que si la entrada es positiva, se pasa directamente como salida; si es negativa, la salida es cero. La función ReLU ayuda a la red a aprender representaciones no lineales. Esto significa que introduce no linealidades en las capas ocultas, permitiendo al modelo aprender representaciones más complejas de los datos.\n",
    "- En la segunda capa `Dense` con activación `sigmoid`: Esta función de activación se utiliza comúnmente en la capa de salida de una red neuronal cuando estamos realizando una tarea de **clasificación binaria**. La función sigmoid produce valores en el rango de 0 a 1 y es útil para modelar la probabilidad de que una instancia pertenezca a la clase positiva. Matemáticamente, la función sigmoid es $$ f(x) = \\frac{1}{1 + e^{-x}} $$ La función sigmoid en la capa de salida es apropiada para tareas de **clasificación binaria**, ya que produce una salida entre 0 y 1 que puede interpretarse como la probabilidad de pertenecer a la clase positiva.\n",
    "\n",
    "Estas elecciones de funciones de activación son comunes, pero dependiendo del problema, pueden explorarse otras funciones de activación como `tanh` o `softmax`. Algunos ejemplos de situaciones en las que podrías preferir utilizarlas son:\n",
    "\n",
    "- Función de activación `tanh`:\n",
    "   - Escalado a valores entre -1 y 1: La función `tanh` (tangente hiperbólica) escala las entradas a valores entre -1 y 1. Puede ser útil en capas ocultas de la red cuando se quiere que las salidas estén en un rango simétrico alrededor de cero.\n",
    "   - Problemas con datos centrados alrededor de cero: Cuando tus datos tienen una media cercana a cero, `tanh` puede ayudar a la convergencia más rápida de la red.\n",
    "\n",
    "```{python}\n",
    "   from keras.layers import Dense\n",
    "   model.add(Dense(units=64, activation='tanh', input_dim=input_size))\n",
    "```\n",
    "\n",
    "2. Función de Activación `softmax`: se utiliza comúnmente en la capa de salida de redes neuronales para problemas de **clasificación multiclase**. Produce una distribución de probabilidad sobre todas las clases, lo que es útil cuando tienes más de dos clases en tu problema. Esta función asegura que la suma de las salidas sea igual a 1, lo que facilita la interpretación como probabilidades.\n",
    "\n",
    "```{python}\n",
    "   from keras.layers import Dense\n",
    "   model.add(Dense(units=num_classes, activation='softmax'))\n",
    "```\n",
    "\n",
    "La elección de la función de activación depende siempre del problema específico y de las características de los datos. Es necesario experimentar con diferentes funciones de activación y evaluar su rendimiento en un conjunto de validación para determinar cuál funciona mejor en cada caso particular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReHUHLiuHFXC"
   },
   "source": [
    "\n",
    "## Compilación del modelo\n",
    "\n",
    "Después de definir el modelo, necesitas **compilarlo** con un **optimizador**, una **función de pérdida** y, opcionalmente, métricas. Esto se realiza utilizando el método compile.\n",
    "\n",
    "```{python}\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "De nuevo, la elección del optimizador y la función de pérdida en un modelo de red neuronal depende del tipo de problema que estás abordando. En el caso de un problema de clasificación binaria, una combinación comúnmente utilizada es `adam` como optimizador y `binary_crossentropy` como función de pérdida, aunque también son comunes otros optimizadores como `rmspop`.\n",
    "\n",
    "- El optimizador Adam es una variante del descenso de gradiente estocástico que combina las ideas del descenso de gradiente con momentum y el método de estimación adaptativa de la tasa de aprendizaje (Adagrad). Es una opción sólida para comenzar en muchos casos y tiende a converger rápidamente.\n",
    "- La función de pérdida de entropía cruzada binaria es apropiada para problemas de clasificación binaria, donde la salida de la red es una probabilidad entre 0 y 1. Esta función de pérdida mide la discrepancia entre las predicciones del modelo y las etiquetas reales. Para problemas de clasificación binaria, es una elección común debido a su capacidad para penalizar de manera efectiva las predicciones incorrectas.\n",
    "\n",
    "### Optimizadores\n",
    "\n",
    "Entre los optimizadores más comunes en Keras encontramos:\n",
    "\n",
    "- SGD (Stochastic Gradient Descent) `sgd`: Este es el optimizador de descenso de gradiente estocástico básico. Es simple y a menudo se utiliza como punto de partida.\n",
    "\n",
    "```\n",
    "from keras.optimizers import SGD\n",
    "optimizer = SGD(lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "- Adam (Adaptive Moment Estimation) `adam`: Combina conceptos de RMSprop y momento. Es ampliamente utilizado debido a su rendimiento general.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "\n",
    "- RMSprop (Root Mean Square Propagation) `rmsprop`: Ajusta la tasa de aprendizaje individualmente para cada parámetro, ayudando en problemas donde las características de los datos pueden variar en magnitud.\n",
    "\n",
    "```\n",
    "from keras.optimizers import RMSprop\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "```\n",
    "\n",
    "- Adagrad `adagrad`: Ajusta la tasa de aprendizaje para cada parámetro según la frecuencia con la que ese parámetro ha sido actualizado en el pasado.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Adagrad\n",
    "optimizer = Adagrad(lr=0.01)\n",
    "```\n",
    "\n",
    "- Adadelta `adadelta`: Similar a Adagrad, pero intenta resolver algunos problemas relacionados con la tasa de aprendizaje que disminuye rápidamente.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Adadelta\n",
    "optimizer = Adadelta()\n",
    "```\n",
    "\n",
    "- Nadam `nadam`: Una variante de Adam que utiliza Nesterov Accelerated Gradient.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Nadam\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "\n",
    "### Funciones de pérdida\n",
    "\n",
    "Para problemas de clasificación en Keras, las funciones de pérdida más comunes son:\n",
    "\n",
    "- Binary Crossentropy `binary_crossentropy`: Para problemas de clasificación binaria.\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- Categorical Crossentropy `categorical_crossentropy`: Para problemas de clasificación multiclase donde las etiquetas son categóricas (one-hot encoded).\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- Sparse Categorical Crossentropy `sparse_categorical_crossentropy`: Similar a Categorical Crossentropy, pero útil cuando las etiquetas son enteros en lugar de codificación one-hot.\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2XI8s5kHNlQ"
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "El **entrenamiento** del modelo se realiza utilizando el método `fit`, proporcionando datos de entrada y etiquetas.\n",
    "\n",
    "```{python}\n",
    "model.fit(training_data, training_labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels))\n",
    "```\n",
    "\n",
    "Donde cada parámetro representa:\n",
    "\n",
    "- `training_data`: Este parámetro representa los datos de entrenamiento que se utilizarán para entrenar el modelo. Tiene que ser un conjunto de datos que incluya ejemplos de entrada para los cuales conoces las salidas esperadas (etiquetas).\n",
    "- `training_labels`: Corresponden a las etiquetas asociadas a los datos de entrenamiento. Cada ejemplo en `training_data` tiene que tener una etiqueta correspondiente en `training_labels`. Estas etiquetas son las salidas esperadas del modelo durante el entrenamiento.\n",
    "- `epochs`: Este parámetro especifica la cantidad de épocas, es decir, la cantidad de veces que el modelo pasará por todo el conjunto de datos de entrenamiento durante el proceso de entrenamiento. Cada época completa implica una pasada hacia adelante (forward pass) y una pasada hacia atrás (backward pass) a través de todos los datos de entrenamiento.\n",
    "- `batch_size`: Representa el tamaño del lote (batch size). Durante el entrenamiento, los datos de entrenamiento se dividen en lotes más pequeños, y el modelo se actualiza después de procesar cada lote. El tamaño del lote afecta la velocidad de entrenamiento y la memoria necesaria.\n",
    "- `validation_data`: Especifica un conjunto de datos de validación que se utiliza para evaluar el rendimiento del modelo después de cada época. `validation_data` consiste en datos de validación (`val_data`) y las etiquetas correspondientes (`val_labels`). Este conjunto no se utiliza para entrenar el modelo, pero sirve para monitorear la capacidad del modelo para generalizar a datos no vistos durante el entrenamiento. También podría indicarse el porcentaje de datos del conjunto de entrenamiento que se quiere utilizar para la validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjIaJ-06HT5L"
   },
   "source": [
    "## Realizar predicciones\n",
    "\n",
    "Después de entrenar el modelo, puedes realizar **predicciones en nuevos datos** utilizando el método `predict` y **evaluar** los resultados usando el método `evaluate`.\n",
    "\n",
    "```\n",
    "predictions = model.predict(new_data)\n",
    "accuracy = model.evaluate(test_data, test_labels)[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObUxDPArHs4-"
   },
   "source": [
    "# Problema de clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes trozos de código veremos como entrenar una red neuronal para resolver un problema de clasificación binaria. Para ello comenzaremos generando un conjunto de datos artificial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "8ERoZmHcBra2",
    "outputId": "131f50d0-d375-4995-ba63-185a669609ed"
   },
   "outputs": [],
   "source": [
    "# Generar un conjunto de datos con dos círculos concéntricos\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1, random_state=33)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Configurar subgráficos\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Subplot para el conjunto de datos completo\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "axs[0].set_title(\"Conjunto de datos completo\")\n",
    "axs[0].set_xlabel(\"Variable 1\")\n",
    "axs[0].set_ylabel(\"Variable 2\")\n",
    "\n",
    "# Subplot para el conjunto de entrenamiento\n",
    "axs[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "axs[1].set_title(\"Conjunto de entrenamiento\")\n",
    "axs[1].set_xlabel(\"Variable 1\")\n",
    "axs[1].set_ylabel(\"Variable 2\")\n",
    "\n",
    "# Subplot para el conjunto de prueba\n",
    "axs[2].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "axs[2].set_title(\"Conjunto de test\")\n",
    "axs[2].set_xlabel(\"Variable 1\")\n",
    "axs[2].set_ylabel(\"Variable 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQy_TAV5Mkh_",
    "outputId": "baa09d36-34c7-453d-b07f-e218afc0326a"
   },
   "outputs": [],
   "source": [
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "np.random.seed(94) # para que sea reproducible\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "oQJBkM7ODX6a",
    "outputId": "3d3e7442-6b6a-4bb4-ce68-9545641dca8e"
   },
   "outputs": [],
   "source": [
    "# Visualizar la frontera de decisión\n",
    "def plot_decision_boundary(X, y, model, scaler):\n",
    "    h = .02  # Tamaño de paso en la malla\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "    plt.title(\"Fronteras de decisión\")\n",
    "    plt.xlabel(\"Variable 1\")\n",
    "    plt.ylabel(\"Variable 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar la frontera de decisión\n",
    "plot_decision_boundary(X_test, y_test, model, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-1PIFAgM7E3"
   },
   "source": [
    "Los valores de **loss** y **accuracy** son métricas que nos ayudan a evaluar el rendimiento de tu modelo en el conjunto de datos de prueba.\n",
    "- Loss (Pérdida): Es una medida de cuánto se equivoca el modelo en sus predicciones. Idealmente, quieres que la pérdida sea lo más baja posible.\n",
    "- Accuracy (Precisión): Es la proporción de predicciones correctas en relación con el total de predicciones. Cuanto más alto sea el valor, mejor.\n",
    "\n",
    "La diferencia entre ambas radica en que la pérdida es una medida cuantitativa de la diferencia entre las predicciones del modelo y las verdaderas etiquetas mientras que la accuracy es una medida percentual que mide la proporción de predicciones correctas en relación con el total de predicciones.\n",
    "\n",
    "Para obtener una evaluación más detallada, podemos utilizar otras métricas y visualizaciones, como la matriz de confusión y el informe de clasificación. Estos te proporcionarán información sobre cómo el modelo se desempeña en cada clase (positiva y negativa) y te ayudarán a identificar posibles problemas, como desequilibrios de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mEzghnSNIYL",
    "outputId": "95dff42a-aa19-4667-bfff-5167666caef4"
   },
   "outputs": [],
   "source": [
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convertir las predicciones a etiquetas binarias (0 o 1)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, binary_predictions))\n",
    "\n",
    "# Imprimir el informe de clasificación\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxUGCtPuONCv"
   },
   "source": [
    "## 🏋🏻 Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiWZbTAFMA2i"
   },
   "source": [
    "En este ejercicio deberás replicar el ejemplo básico de clasificación binaria utilizando una red neuronal con variables numéricas. En este caso, utilizaremos el conjunto de datos de [cáncer de mama de sklearn](https://archive.ics.uci.edu/dataset/14/breast+cancer) como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtRrs7y8L8cl",
    "outputId": "01bea4ca-8743-4c27-b885-413f789ca700"
   },
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos de cáncer de mama\n",
    "data = load_breast_cancer()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No haría falta, pero por tener una idea de los datos\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza aquí el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxXFKTKyHq_y"
   },
   "source": [
    "# Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rULBxFLVxpqq"
   },
   "source": [
    "Las redes neuronales sufren gran riesgo de overfitting. Una de las principales casusas es, por ejemplo, que al tener tantos parámetros si el conjunto de datos no es lo suficientemente grande éstos se adaptarán en exceso a las muestras que han visto.\n",
    "\n",
    "Cuando las métricas muestran la siguiente tendencia:\n",
    "- Loss aumenta en validation y disminuye en training\n",
    "- Accuracy aumenta en training y disminuye en validation\n",
    "Entonces el modelo muestra sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mejAJWcUGR2H"
   },
   "source": [
    "Vamos a ver un ejemplo que nos permitirá detectar cuando nuestro modelo sufre sobreajuste. Para ello utilizaremos el conjunto de [datos IMDB](https://keras.io/api/datasets/imdb/). Este conjunto de datos de IMDB (Internet Movie Database) es comúnmente utilizado en tareas de procesamiento de lenguaje natural (NLP) y clasificación de texto como primer ejemplo. Se trata de una colección de reseñas de películas recopiladas de la plataforma IMDb. Cada reseña está etiquetada como positiva o negativa, lo que lo convierte en un conjunto de datos de clasificación binaria. En total el conjunto de datos contiene 50000 reseñas de las cuales 25000 están reservadas para el entrenamiento y las restantes 25000 para la evaluación. Ambos subconjuntos se dividien al 50% en reseñas positivas y negativas. El conjunto de datos viene preinstalado en `tensorflow.keras.datasets` así que podemos descargarlo como se muestra a continuación (esto descargará unos 80MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLAW8vbkw917"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeyEISUHHMo9"
   },
   "source": [
    "Si observamos por ejemplo la primera reseña vemos que contiene un array de números. Esto se debe a que las reseñas han sido previamente preprocesadas y son representadas por la codificacion numérica en un diccionario de las palabras que contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n7IiKSPxAJ0",
    "outputId": "28c79baf-3664-491c-9631-aa608767bd84"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE5TU2a9HzbZ"
   },
   "source": [
    "Por otro lado, las etiquetas correspondientes serán 0 si las reseñas son negativas y 1 si las reseñas son positivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZjWjyLNxBQJ",
    "outputId": "c9f88c5e-e07a-4be5-aad7-0e549ce2c8cc"
   },
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enXLRQqzH9Oi"
   },
   "source": [
    "Observa que al cargar el conjunto de datos hemos utilizado el parámetro `num_words=10000`. Esto significa que se mantienen solo las 10000 palabras más frecuentes, descartando así palabras raras. Si no fijamos este límite, estaríamos por encima de las 88000 palabras únicas, lo cual impactaría en el teimpo de ejecución del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6nrhFWIxCNY",
    "outputId": "09b24403-8aa4-4c6c-c738-196c3d58716f"
   },
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOe2ZpYFxEj4"
   },
   "source": [
    "Si estamos interesados en hacer la codificación inversa podemos ejecutar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "ymlfpqIxxICf",
    "outputId": "9eeb8e33-f9be-4fc4-a68c-93c2bfbb9904"
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]) # offset de 3 porque 0, 1 y 2 son para padding, comienzo secuencia y desconocido\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5wI7LvxIkZw"
   },
   "source": [
    "Para poder entrenar la red neuronal necesitamos que los datos estén en un formato compatible que la red entienda. Estas listas de enteros representando las reseñas tienen longitudes variables, y las redes neuronales necesitan unas dimensiones fijas. Para unificar las dimensiones haremos un *multi-hot encoding* donde convertiremos cada array en un vector booleano de dimensión 10000. Una vez tengamos esto, ya podemos utilizar una capa `Dense` como primera capa que reciba esto como datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "femFHlsFxMhi"
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSceBtKzxPFm",
    "outputId": "69919df8-33e3-491c-cc0b-3bfe5e89877e"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGPjK6XmxQoV"
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2faTWz6xS_m"
   },
   "source": [
    "Para seguir un proceso train-validate-test, separamos un conjunto de datos para la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzhdTzQTxa2W"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuYqRmSXKaEB"
   },
   "source": [
    "## 🏋🏻 Ejercicio\n",
    "\n",
    "Define ahora el modelo utilizando dos capas densas de 32 neuronas. Entrena el modelo con 20 épocas y un tamaño de batch de 512 objetos. Utiliza el optimizador `rmsprop` y computa la `accuracy` de cada época. ¿Consideras que hay sobreajuste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOsFOUA6xUhU"
   },
   "outputs": [],
   "source": [
    "# Realiza aquí el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8H6wsQTLWZr"
   },
   "source": [
    "## 🏋🏻 Ejercicio\n",
    "\n",
    "Entrena de nuevo un modelo reduciendo el número de nodos en las capas densas a 4, manteniendo el resto de especificaciones indicadas. ¿Se reduce el overfitting? Compara los dos modelos generados, usando las variables `history_original` que almacena el resultado de entrenar el modelo original y `history_smaller_model` que hace lo mismo con el de tamaño reducido. Realiza lo mismo con `history_larger_model` que contendrá 512 en cada capa. ¿Varían los resultados? ¿Varía el tiempo de ejercución?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZvra56_xtNU",
    "outputId": "c4d90edd-da19-4236-ad29-1a9d0384a072"
   },
   "outputs": [],
   "source": [
    "# Realiza aquí el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con los modelos entrenados, podemos realizar la comparación de los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "RlxQIGFsZTBE",
    "outputId": "94e4c23b-7260-4385-d7d9-11257cad8d2a"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que tienes objetos de historial para cada modelo\n",
    "history_original_dict = history_original.history\n",
    "history_smaller_model_dict = history_smaller_model.history\n",
    "history_larger_model_dict = history_larger_model.history\n",
    "\n",
    "# Extraer valores de pérdida y pérdida de validación para cada modelo\n",
    "loss_values_original = history_original_dict[\"loss\"]\n",
    "val_loss_values_original = history_original_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_smaller_model = history_smaller_model_dict[\"loss\"]\n",
    "val_loss_values_smaller_model = history_smaller_model_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_larger_model = history_larger_model_dict[\"loss\"]\n",
    "val_loss_values_larger_model = history_larger_model_dict[\"val_loss\"]\n",
    "\n",
    "# Graficar la pérdida de entrenamiento y validación para cada modelo\n",
    "epochs = range(1, len(loss_values_original) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gráfica para el modelo original\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, loss_values_original, \"bo\", label=\"Pérdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_original, \"b\", label=\"Pérdida de validación\")\n",
    "plt.title(\"Modelo Original\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Gráfica para el modelo más pequeño\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, loss_values_smaller_model, \"bo\", label=\"Pérdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_smaller_model, \"b\", label=\"Pérdida de validación\")\n",
    "plt.title(\"Modelo más Pequeño\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Gráfica para el modelo más grande\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, loss_values_larger_model, \"bo\", label=\"Pérdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_larger_model, \"b\", label=\"Pérdida de validación\")\n",
    "plt.title(\"Modelo más Grande\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mROe7UOVzPUh"
   },
   "source": [
    "## Reducción de sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reducción de sobreajuste es una preocupación clave al desarrollar modelos de aprendizaje profundo, ya que estos pueden aprender patrones demasiado específicos de los datos de entrenamiento que no se generalizan bien a nuevos datos. Para abordar este problema, se utilizan diversas técnicas, entre las que se destacan los regularizadores y la técnica de dropout.\n",
    "\n",
    "Los regularizadores, como L1 y L2, son métodos que penalizan los pesos del modelo durante el proceso de entrenamiento, evitando así que alcancen valores extremadamente altos. Esto ayuda a prevenir el sobreajuste al mantener los pesos del modelo en rangos más manejables.\n",
    "\n",
    "Otra estrategia efectiva es el dropout, que consiste en \"apagar\" aleatoriamente un porcentaje de neuronas durante el entrenamiento. Esto fuerza al modelo a aprender de manera más robusta y a evitar depender demasiado de ciertas neuronas específicas, mejorando así la generalización del modelo.\n",
    "\n",
    "En conjunto, estos enfoques ofrecen herramientas para mitigar el sobreajuste, permitiendo que los modelos aprendan patrones más representativos y sean más efectivos al enfrentarse a datos nunca vistos previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizando los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgYqAzvqzRJx",
    "outputId": "a2f479e1-7e5a-46f1-e08c-078c44a32988"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002), # regularizar\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002), # regularizar\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "np.random.seed(94)\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "pli4aW4baPAv",
    "outputId": "5b1fb917-1bbd-4f2e-d33b-41cb6e3af5c9"
   },
   "outputs": [],
   "source": [
    "history_original_dict = history_original.history\n",
    "history_l2_reg_dict = history_l2_reg.history\n",
    "\n",
    "# Extraer valores de pérdida y pérdida de validación para cada modelo\n",
    "loss_values_original = history_original_dict[\"loss\"]\n",
    "val_loss_values_original = history_original_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_l2_reg = history_l2_reg_dict[\"loss\"]\n",
    "val_loss_values_l2_reg = history_l2_reg_dict[\"val_loss\"]\n",
    "\n",
    "# Graficar la pérdida de entrenamiento y validación para cada modelo\n",
    "epochs = range(1, len(loss_values_original) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gráfica para el modelo original\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values_original, \"bo\", label=\"Pérdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_original, \"b\", label=\"Pérdida de validación\")\n",
    "plt.title(\"Modelo Original\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer límites del eje y entre 0 y 1\n",
    "\n",
    "# Gráfica para el modelo con regularización L2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss_values_l2_reg, \"bo\", label=\"Pérdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_l2_reg, \"b\", label=\"Pérdida de validación\")\n",
    "plt.title(\"Modelo con Regularización L2\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer límites del eje y entre 0 y 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e0nQ4R7zSQo"
   },
   "source": [
    "### Añadiendo dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZGLgxDgzVWJ"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), # dropout\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), # dropout\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_dropout = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aZHqymYak7D"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que tienes objetos de historial para cada modelo\n",
    "history_original_dict = history_original.history\n",
    "history_dropout_dict = history_dropout.history  # Cambio de history_l2_reg a history_dropout\n",
    "\n",
    "# Extraer valores de pérdida y pérdida de validación para cada modelo\n",
    "loss_values_original = history_original_dict[\"loss\"]\n",
    "val_loss_values_original = history_original_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_dropout = history_dropout_dict[\"loss\"]  # Cambio de loss_values_l2_reg a loss_values_dropout\n",
    "val_loss_values_dropout = history_dropout_dict[\"val_loss\"]  # Cambio de val_loss_values_l2_reg a val_loss_values_dropout\n",
    "\n",
    "# Graficar la pérdida de entrenamiento y validación para cada modelo\n",
    "epochs = range(1, len(loss_values_original) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gráfica para el modelo original\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values_original, \"bo\", label=\"Pérdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_original, \"b\", label=\"Pérdida de validación\")\n",
    "plt.title(\"Modelo Original\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer límites del eje y entre 0 y 1\n",
    "\n",
    "# Gráfica para el modelo con Dropout\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss_values_dropout, \"bo\", label=\"Pérdida de entrenamiento\")  # Cambio de loss_values_l2_reg a loss_values_dropout\n",
    "plt.plot(epochs, val_loss_values_dropout, \"b\", label=\"Pérdida de validación\")  # Cambio de val_loss_values_l2_reg a val_loss_values_dropout\n",
    "plt.title(\"Modelo con Dropout\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer límites del eje y entre 0 y 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
