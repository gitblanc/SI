{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer, make_circles, make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import imdb, mnist, reuters\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXlQ9v7i_xBu"
   },
   "source": [
    "# El perceptr贸n\n",
    "\n",
    "El perceptr贸n es un sistema muy simple y podemos programarlo sin gran dificultad. Comenzaremos mostrando c贸mo un perceptr贸n va evolucionando hasta aprender a clasificar los datos. El perceptr贸n realiza la predicci贸n y actualiza los pesos acorde a lo que se define a continuaci贸n:\n",
    "\n",
    "1. **Predicci贸n:** el valor de la clase ($y$) se decide acorde a la siguiente f贸rmula.\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    1 & \\text{si } \\sum_{i=1}^{n} w_i x_i + b > 0 \\\\\n",
    "    0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $y$ es la salida (predicci贸n) del perceptr贸n.\n",
    "- $w_i$ son los pesos asociados a cada entrada $x_i$.\n",
    "- $b$ es el sesgo (bias).\n",
    "- $n$ es el n煤mero de caracter铆sticas de entrada.\n",
    "\n",
    "2. **Actualizaci贸n de pesos:** Una vez predicha la clase se actualiza cada peso de la red ($w_i$) acorde a lo siguiente:\n",
    "$$ w_i = w_i + \\text{tasa de aprendizaje} \\times (y_{\\text{real}} - y_{\\text{predicha}}) \\times x_i $$\n",
    "\n",
    "Donde:\n",
    "- $w_i$ se actualiza para cada caracter铆stica de entrada.\n",
    "- $y_{\\text{real}}$ es la etiqueta real del ejemplo de entrenamiento.\n",
    "- $y_{\\text{predicha}}$ es la predicci贸n del perceptr贸n.\n",
    "\n",
    "El sesgo tambi茅n se actualiza de manera similar:\n",
    "$$b = b + \\text{tasa de aprendizaje} \\times (y_{\\text{real}} - y_{\\text{predicha}})$$\n",
    "\n",
    "A continuaci贸n daremos una implementaci贸n del perceptr贸n donde:\n",
    "\n",
    "- `self.weights` representa $w_i$ (los pesos).\n",
    "- `self.weights[0]` representa el sesgo $b$.\n",
    "- `self.predict(inputs)` calcula $y_{\\text{predicha}}$.\n",
    "- `label` representa $y_{\\text{real}}$.\n",
    "- La actualizaci贸n de pesos se realiza en la funci贸n `train` del perceptr贸n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hp7gFHRADeA"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.01, max_epochs=100):\n",
    "        self.weights = np.random.rand(input_size + 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return 1 if summation > 0 else 0\n",
    "\n",
    "    def train(self, training_inputs, labels, visualize=False):\n",
    "        fig, axes = plt.subplots(1, self.max_epochs, figsize=(20, 6))  # Crear una fila de subfiguras\n",
    "        fig.suptitle('Perceptr贸n - Evoluci贸n por poca', y=1.02)\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "            # Verificar si se ha alcanzado la convergencia antes de mostrar la subfigura\n",
    "            if visualize and all(self.predict(inputs) == label for inputs, label in zip(training_inputs, labels)):\n",
    "                print(f'Convergencia alcanzada en la 茅poca {epoch + 1}')\n",
    "                break\n",
    "\n",
    "            if visualize:\n",
    "                # Visualizar la frontera de decisi贸n despu茅s de cada 茅poca en una subfigura\n",
    "                self.plot_decision_boundary(training_inputs, labels, axes[epoch], epoch + 1, self.max_epochs)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self, inputs, labels, ax, epoch, max_epochs):\n",
    "        ax.clear()\n",
    "        ax.scatter(inputs[:, 0], inputs[:, 1], c=labels, cmap='viridis')\n",
    "        ax.set_xlabel('Caracter铆stica 1')\n",
    "        ax.set_ylabel('Caracter铆stica 2')\n",
    "\n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        Z = np.array([self.predict([x, y]) for x, y in np.c_[xx.ravel(), yy.ravel()]])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "        ax.set_title(f'poca {epoch}/{max_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F0uodYIAc95"
   },
   "source": [
    "Probaremos esta funci贸n para un conjunto de datos linealmente separables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "ICiz-4WZBC-n",
    "outputId": "43d7c77c-2e5c-4cad-ef4e-8ac8bf2ce6b5"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.rand(100, 2)\n",
    "labels = (2 * data[:, 0] + 3 * data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos seg煤n la etiqueta\n",
    "class_0 = data[labels == 0]\n",
    "class_1 = data[labels == 1]\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', label='Class 0')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], marker='o', label='Class 1')\n",
    "\n",
    "# Etiquetas y leyenda\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualization of Binary Classification Data')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gr谩fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnsv_1E0BQve"
   },
   "source": [
    "El objetivo ser铆a que la red neuronal llegara a aprender la siguiente divisi贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "wxl2KDiUBUc1",
    "outputId": "8f6a6122-20c2-4e18-90f2-80265ccc4195"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.rand(100, 2)\n",
    "labels = (2 * data[:, 0] + 3 * data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos seg煤n la etiqueta\n",
    "class_0 = data[labels == 0]\n",
    "class_1 = data[labels == 1]\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', label='Class 0')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], marker='o', label='Class 1')\n",
    "\n",
    "# Dibujar la frontera de decisi贸n (l铆nea recta en este caso)\n",
    "x_decision_boundary = np.linspace(0, 1, 100)\n",
    "y_decision_boundary = (2 - 2 * x_decision_boundary) / 3\n",
    "plt.plot(x_decision_boundary, y_decision_boundary, color='red', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Etiquetas y leyenda\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualization of Binary Classification Data')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gr谩fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIIIbOyPBbjb"
   },
   "source": [
    "Ve谩mos c贸mo se computa utlizando la clase perceptr贸n implementada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "rAIeyRKqBdCL",
    "outputId": "7e158803-007f-4c3c-9d7e-6db25a64c702"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el perceptr贸n con visualizaci贸n\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc23A1U3Bt9L"
   },
   "source": [
    "Ahora probemos en un conjunto de datos diferente. Es importante observar que la inicializaci贸n aleatoria de las variables puede implicar que la red tarde m谩s o menos tiempo en converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X10Isi5xBz_-"
   },
   "outputs": [],
   "source": [
    "# Generar un conjunto de datos m谩s desafiante\n",
    "data, labels = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "komubWpEB2Fk"
   },
   "source": [
    "Ve谩mos como, dependiendo de la inicializaci贸n aleatoria de los pesos, la red tarda m谩s o menos en realizar el aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "eK5mav_rB4sp",
    "outputId": "520bb380-7e33-4624-bdb6-2182547044a3"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "FWzJIwgUB5W9",
    "outputId": "27cf1772-da35-426d-cdde-2e9aba9205fe"
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "8l0ukiKDB6dR",
    "outputId": "7343c9ac-ee65-4ac5-b75d-d4085e168cf3"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "perceptron = Perceptron(input_size=2, max_epochs=10)\n",
    "perceptron.train(data, labels, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR1J7XmkCIkI"
   },
   "source": [
    "# Introducci贸n a `keras`\n",
    "\n",
    "Replicar un perceptr贸n en Keras es bastante sencillo, ya que como hemos visto el perceptr贸n es una red neuronal con una sola capa. En el siguiente ejemplo utilizaremos `Sequential()` para crear un modelo en el que pueden a帽adirse capas seucnecialmente. Como el perceptr贸n solo tiene una capa, esta se a帽ade utilizando `add(Dense(...))` con una sola unidad. En este caso utilizaremos una funci贸n de activaci贸n sigmoide, ya que estamos realizando una tarea de clasificaci贸n binaria.\n",
    "\n",
    "Luego, el modelo se compila con un optimizador (utilizaremos Gradiente Descendente Estoc谩stico), una funci贸n de p茅rdida (en este caso entrop铆a cruzada binaria, adecuada para clasificaci贸n binaria), y una m茅trica (accuracy en este caso). Finalmente, el modelo se entrena con los datos de entrada y etiquetas, y se eval煤a su rendimiento en los mismos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "dAEykaK_DqNp",
    "outputId": "381e570a-53b8-49eb-c73e-4399a7ef8988"
   },
   "outputs": [],
   "source": [
    "# Datos de entrada\n",
    "np.random.seed(1)\n",
    "data = np.random.rand(100, 2)\n",
    "labels = (2 * data[:, 0] + 3 * data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos seg煤n la etiqueta\n",
    "class_0 = data[labels == 0]\n",
    "class_1 = data[labels == 1]\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', label='Class 0')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], marker='o', label='Class 1')\n",
    "\n",
    "# Etiquetas y leyenda\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualizaci贸n de los datos para clasificaci贸n bina')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gr谩fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoSaK8khCOvW",
    "outputId": "88055b90-13c8-481f-a1bb-e2a05ae2cbda"
   },
   "outputs": [],
   "source": [
    "# Crear un modelo secuencial (perceptr贸n)\n",
    "perceptron_model = Sequential()\n",
    "\n",
    "# A帽adir una capa densa (totalmente conectada) con una sola unidad y funci贸n de activaci贸n sigmoid\n",
    "perceptron_model.add(Dense(units=1, input_dim=2, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo con un optimizador (gradiente descendente estoc谩stico), funci贸n de p茅rdida y m茅trica\n",
    "perceptron_model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "perceptron_model.fit(data, labels, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VfrxgVpDyCt"
   },
   "source": [
    "La salida que se ve durante el entrenamiento contiene la siguiente informaci贸n:\n",
    "\n",
    "- Epoch x/epochs: Indica la 茅poca en la que est谩s. Durante cada 茅poca, el modelo pasa por todos los datos de entrenamiento una vez.\n",
    "- x/batches: Indica el n煤mero de lotes (batches) que llevas completado en cada 茅poca y se actualiza din谩micamente. Mediante\n",
    "[==============================], la barra de progreso visual indica cu谩nto del total de lotes se ha completado en la 茅poca actual.\n",
    "- xs xxms/step: Indica que, en promedio, cada paso (o lote) ha tomado xx milisegundos y que la 茅poca en su totalidad ha tomado x segundos.\n",
    "- loss: El valor de la funci贸n de p茅rdida en el conjunto de entrenamiento al final de esta 茅poca. Este valor mide cu谩nto se desv铆an las predicciones del modelo respecto a las etiquetas reales.\n",
    "- accuracy: La precisi贸n del modelo en el conjunto de entrenamiento al final de esta 茅poca. Este valor indica la proporci贸n de ejemplos de entrenamiento que se clasificaron correctamente.\n",
    "- val_loss: El valor de la funci贸n de p茅rdida en el conjunto de validaci贸n al final de esta 茅poca. Es similar a la p茅rdida en el conjunto de entrenamiento, pero se eval煤a en un conjunto de datos no utilizado para el entrenamiento.\n",
    "- val_accuracy: La precisi贸n en el conjunto de validaci贸n al final de esta 茅poca. Al igual que la precisi贸n en el conjunto de entrenamiento, indica la proporci贸n de ejemplos de validaci贸n que se clasificaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrSEs4E0CQ2u"
   },
   "source": [
    "La red neuronal tiene la forma que se muestra a continuaci贸n. Vemos que las capas contienen informaci贸n del tipo `(None, 2)`, que representa el tama帽o del lote (batch size) y el n煤mero de neuronas en esa capa. El `None` en la forma indica que la dimensi贸n correspondiente puede tener cualquier tama帽o. En el contexto del tama帽o del lote (batch size), significa que el modelo puede aceptar datos de entrada con tama帽os de lote variables. Por este motivo, durante el entrenamiento y la predicci贸n, puedes alimentar al modelo lotes de diferentes tama帽os, y la red se adaptar谩 en consecuencia. El tama帽o real del lote se determina durante la ejecuci贸n. El n煤mero 2 en (None, 2) representa la cantidad de neuronas en esa capa espec铆fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "_yYMNUSaCTcb",
    "outputId": "b5d371cb-c9b0-41db-c4b3-684359bfc9ab"
   },
   "outputs": [],
   "source": [
    "plot_model(perceptron_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDplmLzWC3cb",
    "outputId": "1b337450-2f35-4814-94cb-2bfe227b3de1"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo en datos de prueba\n",
    "accuracy = perceptron_model.evaluate(data, labels)[1]\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzhXXcBSDBWN"
   },
   "source": [
    "Normalmente, querr谩s evaluar el modelo en un conjunto de datos de prueba separado para obtener una medida m谩s realista de su rendimiento en datos no vistos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "EebXcliODGrv",
    "outputId": "7eec81cd-5fa2-4816-a2bc-2b1b47cd8624"
   },
   "outputs": [],
   "source": [
    "# Datos de entrada\n",
    "np.random.seed(2)\n",
    "test_data = np.random.rand(100, 2)\n",
    "test_labels = (2 * test_data[:, 0] + 3 * test_data[:, 1] > 2).astype(int)\n",
    "\n",
    "# Dividir los datos en dos conjuntos seg煤n la etiqueta\n",
    "class_0_train = data[labels == 0]\n",
    "class_1_train = data[labels == 1]\n",
    "\n",
    "class_0_test = test_data[test_labels == 0]\n",
    "class_1_test = test_data[test_labels == 1]\n",
    "\n",
    "# Configurar subfiguras\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Subfigura 1: Datos de entrenamiento\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(class_0_train[:, 0], class_0_train[:, 1], marker='o', label='Class 0 (Train)')\n",
    "plt.scatter(class_1_train[:, 0], class_1_train[:, 1], marker='o', label='Class 1 (Train)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "\n",
    "# Subfigura 2: Datos de prueba\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(class_0_test[:, 0], class_0_test[:, 1], marker='o', label='Class 0 (Test)')\n",
    "plt.scatter(class_1_test[:, 0], class_1_test[:, 1], marker='o', label='Class 1 (Test)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gr谩fico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1yiORejBULd",
    "outputId": "801ca1bf-e512-44cb-a4fb-50fc27f29b3a"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de test\n",
    "accuracy = perceptron_model.evaluate(test_data, test_labels)[1]\n",
    "print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rofYRVX6Jaqe"
   },
   "source": [
    "Muchas veces estos modelos requerir谩n un tiempo de ejecuci贸n elevado y no podremos permitirnos entrenarlos dos veces, por lo que en estas situaciones es crucial persistir el modelo generado. Una de las formas en las que podemos hacer esto es generando un archivo `.keras` (tambi茅n puede exportarse a otros formatos como por ejemplo (`.h5`) utilizando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj3lTKTNKQYc"
   },
   "outputs": [],
   "source": [
    "perceptron_model.save(\"modelo.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0E4EPxSKyOR"
   },
   "source": [
    "Estos archivos pueden cargarse luego ejecutando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sf1WvK2GK0LJ",
    "outputId": "2b6a4eea-4362-4b41-f824-603f55f196e4"
   },
   "outputs": [],
   "source": [
    "modelo_cargado = load_model(\"modelo.keras\")\n",
    "modelo_cargado.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7YR93EtE1De"
   },
   "source": [
    "# Desglosando Keras\n",
    "\n",
    "Keras es una biblioteca de alto nivel para construir y entrenar modelos de redes neuronales en Python. Est谩 dise帽ada para ser f谩cil de usar, modular y extensible. Keras proporciona una interfaz de alto nivel para construir modelos neuronales, mientras que internamente utiliza otras bibliotecas como TensorFlow para realizar c谩lculos eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks6tPJNiFU1m"
   },
   "source": [
    "## Definici贸n del modelo\n",
    "\n",
    "En Keras, los **modelos** se definen como una secuencia de capas. El modelo m谩s com煤n es el modelo secuencial, que se crea mediante la clase `Sequential`. Como su propio nombre indica, puedes agregar capas a este modelo de manera secuencial.\n",
    "\n",
    "```{python}\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Crear un modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Agregar capas al modelo\n",
    "model.add(Dense(units=64, activation='relu', input_dim=input_size))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "Las capas son los bloques de construcci贸n fundamentales en un modelo de Keras. Puedes usar diferentes tipos de capas, como capas densas (totalmente conectadas), capas de convoluci贸n, capas recurrentes, etc.\n",
    "\n",
    "```{python}\n",
    "from keras.layers import Dense, Conv2D, LSTM\n",
    "```\n",
    "\n",
    "Las funciones de activaci贸n se aplican a la salida de una capa y le dan no linealidad al modelo. Algunas funciones de activaci贸n comunes son ReLU, Sigmoid y Tanh.\n",
    "\n",
    "```{python}\n",
    "from keras.activations import relu, sigmoid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1lRiEqoG4kx"
   },
   "source": [
    "### Funciones de activaci贸n\n",
    "\n",
    "La funci贸n de activaci贸n de una capa en una red neuronal desempe帽a un papel crucial en la capacidad del modelo para aprender y representar patrones en los datos. En concreto en la definici贸n de la arquitectura del c贸digo anterior hemos utilizado dos capas para definir el modelo y cada una de ellas tiene una funci贸n de activaci贸n distinta.\n",
    "\n",
    "- En la primera capa `Dense` con activaci贸n `relu`: La funci贸n de activaci贸n Rectified Linear Unit es com煤nmente utilizada en capas ocultas de una red neuronal ya que es simple pero efectiva. Matem谩ticamente, ReLU se define como $$f(x) = \\max(0, x)$$ lo cual significa que si la entrada es positiva, se pasa directamente como salida; si es negativa, la salida es cero. La funci贸n ReLU ayuda a la red a aprender representaciones no lineales. Esto significa que introduce no linealidades en las capas ocultas, permitiendo al modelo aprender representaciones m谩s complejas de los datos.\n",
    "- En la segunda capa `Dense` con activaci贸n `sigmoid`: Esta funci贸n de activaci贸n se utiliza com煤nmente en la capa de salida de una red neuronal cuando estamos realizando una tarea de **clasificaci贸n binaria**. La funci贸n sigmoid produce valores en el rango de 0 a 1 y es 煤til para modelar la probabilidad de que una instancia pertenezca a la clase positiva. Matem谩ticamente, la funci贸n sigmoid es $$ f(x) = \\frac{1}{1 + e^{-x}} $$ La funci贸n sigmoid en la capa de salida es apropiada para tareas de **clasificaci贸n binaria**, ya que produce una salida entre 0 y 1 que puede interpretarse como la probabilidad de pertenecer a la clase positiva.\n",
    "\n",
    "Estas elecciones de funciones de activaci贸n son comunes, pero dependiendo del problema, pueden explorarse otras funciones de activaci贸n como `tanh` o `softmax`. Algunos ejemplos de situaciones en las que podr铆as preferir utilizarlas son:\n",
    "\n",
    "- Funci贸n de activaci贸n `tanh`:\n",
    "   - Escalado a valores entre -1 y 1: La funci贸n `tanh` (tangente hiperb贸lica) escala las entradas a valores entre -1 y 1. Puede ser 煤til en capas ocultas de la red cuando se quiere que las salidas est茅n en un rango sim茅trico alrededor de cero.\n",
    "   - Problemas con datos centrados alrededor de cero: Cuando tus datos tienen una media cercana a cero, `tanh` puede ayudar a la convergencia m谩s r谩pida de la red.\n",
    "\n",
    "```{python}\n",
    "   from keras.layers import Dense\n",
    "   model.add(Dense(units=64, activation='tanh', input_dim=input_size))\n",
    "```\n",
    "\n",
    "2. Funci贸n de Activaci贸n `softmax`: se utiliza com煤nmente en la capa de salida de redes neuronales para problemas de **clasificaci贸n multiclase**. Produce una distribuci贸n de probabilidad sobre todas las clases, lo que es 煤til cuando tienes m谩s de dos clases en tu problema. Esta funci贸n asegura que la suma de las salidas sea igual a 1, lo que facilita la interpretaci贸n como probabilidades.\n",
    "\n",
    "```{python}\n",
    "   from keras.layers import Dense\n",
    "   model.add(Dense(units=num_classes, activation='softmax'))\n",
    "```\n",
    "\n",
    "La elecci贸n de la funci贸n de activaci贸n depende siempre del problema espec铆fico y de las caracter铆sticas de los datos. Es necesario experimentar con diferentes funciones de activaci贸n y evaluar su rendimiento en un conjunto de validaci贸n para determinar cu谩l funciona mejor en cada caso particular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReHUHLiuHFXC"
   },
   "source": [
    "\n",
    "## Compilaci贸n del modelo\n",
    "\n",
    "Despu茅s de definir el modelo, necesitas **compilarlo** con un **optimizador**, una **funci贸n de p茅rdida** y, opcionalmente, m茅tricas. Esto se realiza utilizando el m茅todo compile.\n",
    "\n",
    "```{python}\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "De nuevo, la elecci贸n del optimizador y la funci贸n de p茅rdida en un modelo de red neuronal depende del tipo de problema que est谩s abordando. En el caso de un problema de clasificaci贸n binaria, una combinaci贸n com煤nmente utilizada es `adam` como optimizador y `binary_crossentropy` como funci贸n de p茅rdida, aunque tambi茅n son comunes otros optimizadores como `rmspop`.\n",
    "\n",
    "- El optimizador Adam es una variante del descenso de gradiente estoc谩stico que combina las ideas del descenso de gradiente con momentum y el m茅todo de estimaci贸n adaptativa de la tasa de aprendizaje (Adagrad). Es una opci贸n s贸lida para comenzar en muchos casos y tiende a converger r谩pidamente.\n",
    "- La funci贸n de p茅rdida de entrop铆a cruzada binaria es apropiada para problemas de clasificaci贸n binaria, donde la salida de la red es una probabilidad entre 0 y 1. Esta funci贸n de p茅rdida mide la discrepancia entre las predicciones del modelo y las etiquetas reales. Para problemas de clasificaci贸n binaria, es una elecci贸n com煤n debido a su capacidad para penalizar de manera efectiva las predicciones incorrectas.\n",
    "\n",
    "### Optimizadores\n",
    "\n",
    "Entre los optimizadores m谩s comunes en Keras encontramos:\n",
    "\n",
    "- SGD (Stochastic Gradient Descent) `sgd`: Este es el optimizador de descenso de gradiente estoc谩stico b谩sico. Es simple y a menudo se utiliza como punto de partida.\n",
    "\n",
    "```\n",
    "from keras.optimizers import SGD\n",
    "optimizer = SGD(lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "- Adam (Adaptive Moment Estimation) `adam`: Combina conceptos de RMSprop y momento. Es ampliamente utilizado debido a su rendimiento general.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "\n",
    "- RMSprop (Root Mean Square Propagation) `rmsprop`: Ajusta la tasa de aprendizaje individualmente para cada par谩metro, ayudando en problemas donde las caracter铆sticas de los datos pueden variar en magnitud.\n",
    "\n",
    "```\n",
    "from keras.optimizers import RMSprop\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "```\n",
    "\n",
    "- Adagrad `adagrad`: Ajusta la tasa de aprendizaje para cada par谩metro seg煤n la frecuencia con la que ese par谩metro ha sido actualizado en el pasado.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Adagrad\n",
    "optimizer = Adagrad(lr=0.01)\n",
    "```\n",
    "\n",
    "- Adadelta `adadelta`: Similar a Adagrad, pero intenta resolver algunos problemas relacionados con la tasa de aprendizaje que disminuye r谩pidamente.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Adadelta\n",
    "optimizer = Adadelta()\n",
    "```\n",
    "\n",
    "- Nadam `nadam`: Una variante de Adam que utiliza Nesterov Accelerated Gradient.\n",
    "\n",
    "```\n",
    "from keras.optimizers import Nadam\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "\n",
    "### Funciones de p茅rdida\n",
    "\n",
    "Para problemas de clasificaci贸n en Keras, las funciones de p茅rdida m谩s comunes son:\n",
    "\n",
    "- Binary Crossentropy `binary_crossentropy`: Para problemas de clasificaci贸n binaria.\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- Categorical Crossentropy `categorical_crossentropy`: Para problemas de clasificaci贸n multiclase donde las etiquetas son categ贸ricas (one-hot encoded).\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- Sparse Categorical Crossentropy `sparse_categorical_crossentropy`: Similar a Categorical Crossentropy, pero 煤til cuando las etiquetas son enteros en lugar de codificaci贸n one-hot.\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2XI8s5kHNlQ"
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "El **entrenamiento** del modelo se realiza utilizando el m茅todo `fit`, proporcionando datos de entrada y etiquetas.\n",
    "\n",
    "```{python}\n",
    "model.fit(training_data, training_labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels))\n",
    "```\n",
    "\n",
    "Donde cada par谩metro representa:\n",
    "\n",
    "- `training_data`: Este par谩metro representa los datos de entrenamiento que se utilizar谩n para entrenar el modelo. Tiene que ser un conjunto de datos que incluya ejemplos de entrada para los cuales conoces las salidas esperadas (etiquetas).\n",
    "- `training_labels`: Corresponden a las etiquetas asociadas a los datos de entrenamiento. Cada ejemplo en `training_data` tiene que tener una etiqueta correspondiente en `training_labels`. Estas etiquetas son las salidas esperadas del modelo durante el entrenamiento.\n",
    "- `epochs`: Este par谩metro especifica la cantidad de 茅pocas, es decir, la cantidad de veces que el modelo pasar谩 por todo el conjunto de datos de entrenamiento durante el proceso de entrenamiento. Cada 茅poca completa implica una pasada hacia adelante (forward pass) y una pasada hacia atr谩s (backward pass) a trav茅s de todos los datos de entrenamiento.\n",
    "- `batch_size`: Representa el tama帽o del lote (batch size). Durante el entrenamiento, los datos de entrenamiento se dividen en lotes m谩s peque帽os, y el modelo se actualiza despu茅s de procesar cada lote. El tama帽o del lote afecta la velocidad de entrenamiento y la memoria necesaria.\n",
    "- `validation_data`: Especifica un conjunto de datos de validaci贸n que se utiliza para evaluar el rendimiento del modelo despu茅s de cada 茅poca. `validation_data` consiste en datos de validaci贸n (`val_data`) y las etiquetas correspondientes (`val_labels`). Este conjunto no se utiliza para entrenar el modelo, pero sirve para monitorear la capacidad del modelo para generalizar a datos no vistos durante el entrenamiento. Tambi茅n podr铆a indicarse el porcentaje de datos del conjunto de entrenamiento que se quiere utilizar para la validaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjIaJ-06HT5L"
   },
   "source": [
    "## Realizar predicciones\n",
    "\n",
    "Despu茅s de entrenar el modelo, puedes realizar **predicciones en nuevos datos** utilizando el m茅todo `predict` y **evaluar** los resultados usando el m茅todo `evaluate`.\n",
    "\n",
    "```\n",
    "predictions = model.predict(new_data)\n",
    "accuracy = model.evaluate(test_data, test_labels)[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObUxDPArHs4-"
   },
   "source": [
    "# Problema de clasificaci贸n binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes trozos de c贸digo veremos como entrenar una red neuronal para resolver un problema de clasificaci贸n binaria. Para ello comenzaremos generando un conjunto de datos artificial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "8ERoZmHcBra2",
    "outputId": "131f50d0-d375-4995-ba63-185a669609ed"
   },
   "outputs": [],
   "source": [
    "# Generar un conjunto de datos con dos c铆rculos conc茅ntricos\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1, random_state=33)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Configurar subgr谩ficos\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Subplot para el conjunto de datos completo\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "axs[0].set_title(\"Conjunto de datos completo\")\n",
    "axs[0].set_xlabel(\"Variable 1\")\n",
    "axs[0].set_ylabel(\"Variable 2\")\n",
    "\n",
    "# Subplot para el conjunto de entrenamiento\n",
    "axs[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "axs[1].set_title(\"Conjunto de entrenamiento\")\n",
    "axs[1].set_xlabel(\"Variable 1\")\n",
    "axs[1].set_ylabel(\"Variable 2\")\n",
    "\n",
    "# Subplot para el conjunto de prueba\n",
    "axs[2].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "axs[2].set_title(\"Conjunto de test\")\n",
    "axs[2].set_xlabel(\"Variable 1\")\n",
    "axs[2].set_ylabel(\"Variable 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQy_TAV5Mkh_",
    "outputId": "baa09d36-34c7-453d-b07f-e218afc0326a"
   },
   "outputs": [],
   "source": [
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "np.random.seed(94) # para que sea reproducible\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "oQJBkM7ODX6a",
    "outputId": "3d3e7442-6b6a-4bb4-ce68-9545641dca8e"
   },
   "outputs": [],
   "source": [
    "# Visualizar la frontera de decisi贸n\n",
    "def plot_decision_boundary(X, y, model, scaler):\n",
    "    h = .02  # Tama帽o de paso en la malla\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k', marker='o', s=50)\n",
    "    plt.title(\"Fronteras de decisi贸n\")\n",
    "    plt.xlabel(\"Variable 1\")\n",
    "    plt.ylabel(\"Variable 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar la frontera de decisi贸n\n",
    "plot_decision_boundary(X_test, y_test, model, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-1PIFAgM7E3"
   },
   "source": [
    "Los valores de **loss** y **accuracy** son m茅tricas que nos ayudan a evaluar el rendimiento de tu modelo en el conjunto de datos de prueba.\n",
    "- Loss (P茅rdida): Es una medida de cu谩nto se equivoca el modelo en sus predicciones. Idealmente, quieres que la p茅rdida sea lo m谩s baja posible.\n",
    "- Accuracy (Precisi贸n): Es la proporci贸n de predicciones correctas en relaci贸n con el total de predicciones. Cuanto m谩s alto sea el valor, mejor.\n",
    "\n",
    "La diferencia entre ambas radica en que la p茅rdida es una medida cuantitativa de la diferencia entre las predicciones del modelo y las verdaderas etiquetas mientras que la accuracy es una medida percentual que mide la proporci贸n de predicciones correctas en relaci贸n con el total de predicciones.\n",
    "\n",
    "Para obtener una evaluaci贸n m谩s detallada, podemos utilizar otras m茅tricas y visualizaciones, como la matriz de confusi贸n y el informe de clasificaci贸n. Estos te proporcionar谩n informaci贸n sobre c贸mo el modelo se desempe帽a en cada clase (positiva y negativa) y te ayudar谩n a identificar posibles problemas, como desequilibrios de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mEzghnSNIYL",
    "outputId": "95dff42a-aa19-4667-bfff-5167666caef4"
   },
   "outputs": [],
   "source": [
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convertir las predicciones a etiquetas binarias (0 o 1)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Imprimir la matriz de confusi贸n\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, binary_predictions))\n",
    "\n",
    "# Imprimir el informe de clasificaci贸n\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxUGCtPuONCv"
   },
   "source": [
    "##  Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiWZbTAFMA2i"
   },
   "source": [
    "En este ejercicio deber谩s replicar el ejemplo b谩sico de clasificaci贸n binaria utilizando una red neuronal con variables num茅ricas. En este caso, utilizaremos el conjunto de datos de [c谩ncer de mama de sklearn](https://archive.ics.uci.edu/dataset/14/breast+cancer) como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtRrs7y8L8cl",
    "outputId": "01bea4ca-8743-4c27-b885-413f789ca700"
   },
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos de c谩ncer de mama\n",
    "data = load_breast_cancer()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No har铆a falta, pero por tener una idea de los datos\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza aqu铆 el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxXFKTKyHq_y"
   },
   "source": [
    "# Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rULBxFLVxpqq"
   },
   "source": [
    "Las redes neuronales sufren gran riesgo de overfitting. Una de las principales casusas es, por ejemplo, que al tener tantos par谩metros si el conjunto de datos no es lo suficientemente grande 茅stos se adaptar谩n en exceso a las muestras que han visto.\n",
    "\n",
    "Cuando las m茅tricas muestran la siguiente tendencia:\n",
    "- Loss aumenta en validation y disminuye en training\n",
    "- Accuracy aumenta en training y disminuye en validation\n",
    "Entonces el modelo muestra sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mejAJWcUGR2H"
   },
   "source": [
    "Vamos a ver un ejemplo que nos permitir谩 detectar cuando nuestro modelo sufre sobreajuste. Para ello utilizaremos el conjunto de [datos IMDB](https://keras.io/api/datasets/imdb/). Este conjunto de datos de IMDB (Internet Movie Database) es com煤nmente utilizado en tareas de procesamiento de lenguaje natural (NLP) y clasificaci贸n de texto como primer ejemplo. Se trata de una colecci贸n de rese帽as de pel铆culas recopiladas de la plataforma IMDb. Cada rese帽a est谩 etiquetada como positiva o negativa, lo que lo convierte en un conjunto de datos de clasificaci贸n binaria. En total el conjunto de datos contiene 50000 rese帽as de las cuales 25000 est谩n reservadas para el entrenamiento y las restantes 25000 para la evaluaci贸n. Ambos subconjuntos se dividien al 50% en rese帽as positivas y negativas. El conjunto de datos viene preinstalado en `tensorflow.keras.datasets` as铆 que podemos descargarlo como se muestra a continuaci贸n (esto descargar谩 unos 80MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLAW8vbkw917"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeyEISUHHMo9"
   },
   "source": [
    "Si observamos por ejemplo la primera rese帽a vemos que contiene un array de n煤meros. Esto se debe a que las rese帽as han sido previamente preprocesadas y son representadas por la codificacion num茅rica en un diccionario de las palabras que contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n7IiKSPxAJ0",
    "outputId": "28c79baf-3664-491c-9631-aa608767bd84"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE5TU2a9HzbZ"
   },
   "source": [
    "Por otro lado, las etiquetas correspondientes ser谩n 0 si las rese帽as son negativas y 1 si las rese帽as son positivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZjWjyLNxBQJ",
    "outputId": "c9f88c5e-e07a-4be5-aad7-0e549ce2c8cc"
   },
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enXLRQqzH9Oi"
   },
   "source": [
    "Observa que al cargar el conjunto de datos hemos utilizado el par谩metro `num_words=10000`. Esto significa que se mantienen solo las 10000 palabras m谩s frecuentes, descartando as铆 palabras raras. Si no fijamos este l铆mite, estar铆amos por encima de las 88000 palabras 煤nicas, lo cual impactar铆a en el teimpo de ejecuci贸n del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6nrhFWIxCNY",
    "outputId": "09b24403-8aa4-4c6c-c738-196c3d58716f"
   },
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOe2ZpYFxEj4"
   },
   "source": [
    "Si estamos interesados en hacer la codificaci贸n inversa podemos ejecutar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "ymlfpqIxxICf",
    "outputId": "9eeb8e33-f9be-4fc4-a68c-93c2bfbb9904"
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]) # offset de 3 porque 0, 1 y 2 son para padding, comienzo secuencia y desconocido\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5wI7LvxIkZw"
   },
   "source": [
    "Para poder entrenar la red neuronal necesitamos que los datos est茅n en un formato compatible que la red entienda. Estas listas de enteros representando las rese帽as tienen longitudes variables, y las redes neuronales necesitan unas dimensiones fijas. Para unificar las dimensiones haremos un *multi-hot encoding* donde convertiremos cada array en un vector booleano de dimensi贸n 10000. Una vez tengamos esto, ya podemos utilizar una capa `Dense` como primera capa que reciba esto como datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "femFHlsFxMhi"
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSceBtKzxPFm",
    "outputId": "69919df8-33e3-491c-cc0b-3bfe5e89877e"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGPjK6XmxQoV"
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2faTWz6xS_m"
   },
   "source": [
    "Para seguir un proceso train-validate-test, separamos un conjunto de datos para la validaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzhdTzQTxa2W"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuYqRmSXKaEB"
   },
   "source": [
    "##  Ejercicio\n",
    "\n",
    "Define ahora el modelo utilizando dos capas densas de 32 neuronas. Entrena el modelo con 20 茅pocas y un tama帽o de batch de 512 objetos. Utiliza el optimizador `rmsprop` y computa la `accuracy` de cada 茅poca. 驴Consideras que hay sobreajuste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOsFOUA6xUhU"
   },
   "outputs": [],
   "source": [
    "# Realiza aqu铆 el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8H6wsQTLWZr"
   },
   "source": [
    "##  Ejercicio\n",
    "\n",
    "Entrena de nuevo un modelo reduciendo el n煤mero de nodos en las capas densas a 4, manteniendo el resto de especificaciones indicadas. 驴Se reduce el overfitting? Compara los dos modelos generados, usando las variables `history_original` que almacena el resultado de entrenar el modelo original y `history_smaller_model` que hace lo mismo con el de tama帽o reducido. Realiza lo mismo con `history_larger_model` que contendr谩 512 en cada capa. 驴Var铆an los resultados? 驴Var铆a el tiempo de ejercuci贸n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZvra56_xtNU",
    "outputId": "c4d90edd-da19-4236-ad29-1a9d0384a072"
   },
   "outputs": [],
   "source": [
    "# Realiza aqu铆 el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con los modelos entrenados, podemos realizar la comparaci贸n de los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "RlxQIGFsZTBE",
    "outputId": "94e4c23b-7260-4385-d7d9-11257cad8d2a"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que tienes objetos de historial para cada modelo\n",
    "history_original_dict = history_original.history\n",
    "history_smaller_model_dict = history_smaller_model.history\n",
    "history_larger_model_dict = history_larger_model.history\n",
    "\n",
    "# Extraer valores de p茅rdida y p茅rdida de validaci贸n para cada modelo\n",
    "loss_values_original = history_original_dict[\"loss\"]\n",
    "val_loss_values_original = history_original_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_smaller_model = history_smaller_model_dict[\"loss\"]\n",
    "val_loss_values_smaller_model = history_smaller_model_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_larger_model = history_larger_model_dict[\"loss\"]\n",
    "val_loss_values_larger_model = history_larger_model_dict[\"val_loss\"]\n",
    "\n",
    "# Graficar la p茅rdida de entrenamiento y validaci贸n para cada modelo\n",
    "epochs = range(1, len(loss_values_original) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gr谩fica para el modelo original\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, loss_values_original, \"bo\", label=\"P茅rdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_original, \"b\", label=\"P茅rdida de validaci贸n\")\n",
    "plt.title(\"Modelo Original\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Gr谩fica para el modelo m谩s peque帽o\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, loss_values_smaller_model, \"bo\", label=\"P茅rdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_smaller_model, \"b\", label=\"P茅rdida de validaci贸n\")\n",
    "plt.title(\"Modelo m谩s Peque帽o\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Gr谩fica para el modelo m谩s grande\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, loss_values_larger_model, \"bo\", label=\"P茅rdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_larger_model, \"b\", label=\"P茅rdida de validaci贸n\")\n",
    "plt.title(\"Modelo m谩s Grande\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mROe7UOVzPUh"
   },
   "source": [
    "## Reducci贸n de sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reducci贸n de sobreajuste es una preocupaci贸n clave al desarrollar modelos de aprendizaje profundo, ya que estos pueden aprender patrones demasiado espec铆ficos de los datos de entrenamiento que no se generalizan bien a nuevos datos. Para abordar este problema, se utilizan diversas t茅cnicas, entre las que se destacan los regularizadores y la t茅cnica de dropout.\n",
    "\n",
    "Los regularizadores, como L1 y L2, son m茅todos que penalizan los pesos del modelo durante el proceso de entrenamiento, evitando as铆 que alcancen valores extremadamente altos. Esto ayuda a prevenir el sobreajuste al mantener los pesos del modelo en rangos m谩s manejables.\n",
    "\n",
    "Otra estrategia efectiva es el dropout, que consiste en \"apagar\" aleatoriamente un porcentaje de neuronas durante el entrenamiento. Esto fuerza al modelo a aprender de manera m谩s robusta y a evitar depender demasiado de ciertas neuronas espec铆ficas, mejorando as铆 la generalizaci贸n del modelo.\n",
    "\n",
    "En conjunto, estos enfoques ofrecen herramientas para mitigar el sobreajuste, permitiendo que los modelos aprendan patrones m谩s representativos y sean m谩s efectivos al enfrentarse a datos nunca vistos previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizando los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgYqAzvqzRJx",
    "outputId": "a2f479e1-7e5a-46f1-e08c-078c44a32988"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002), # regularizar\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002), # regularizar\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "np.random.seed(94)\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "pli4aW4baPAv",
    "outputId": "5b1fb917-1bbd-4f2e-d33b-41cb6e3af5c9"
   },
   "outputs": [],
   "source": [
    "history_original_dict = history_original.history\n",
    "history_l2_reg_dict = history_l2_reg.history\n",
    "\n",
    "# Extraer valores de p茅rdida y p茅rdida de validaci贸n para cada modelo\n",
    "loss_values_original = history_original_dict[\"loss\"]\n",
    "val_loss_values_original = history_original_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_l2_reg = history_l2_reg_dict[\"loss\"]\n",
    "val_loss_values_l2_reg = history_l2_reg_dict[\"val_loss\"]\n",
    "\n",
    "# Graficar la p茅rdida de entrenamiento y validaci贸n para cada modelo\n",
    "epochs = range(1, len(loss_values_original) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gr谩fica para el modelo original\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values_original, \"bo\", label=\"P茅rdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_original, \"b\", label=\"P茅rdida de validaci贸n\")\n",
    "plt.title(\"Modelo Original\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer l铆mites del eje y entre 0 y 1\n",
    "\n",
    "# Gr谩fica para el modelo con regularizaci贸n L2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss_values_l2_reg, \"bo\", label=\"P茅rdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_l2_reg, \"b\", label=\"P茅rdida de validaci贸n\")\n",
    "plt.title(\"Modelo con Regularizaci贸n L2\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer l铆mites del eje y entre 0 y 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e0nQ4R7zSQo"
   },
   "source": [
    "### A帽adiendo dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZGLgxDgzVWJ"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), # dropout\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), # dropout\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_dropout = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aZHqymYak7D"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que tienes objetos de historial para cada modelo\n",
    "history_original_dict = history_original.history\n",
    "history_dropout_dict = history_dropout.history  # Cambio de history_l2_reg a history_dropout\n",
    "\n",
    "# Extraer valores de p茅rdida y p茅rdida de validaci贸n para cada modelo\n",
    "loss_values_original = history_original_dict[\"loss\"]\n",
    "val_loss_values_original = history_original_dict[\"val_loss\"]\n",
    "\n",
    "loss_values_dropout = history_dropout_dict[\"loss\"]  # Cambio de loss_values_l2_reg a loss_values_dropout\n",
    "val_loss_values_dropout = history_dropout_dict[\"val_loss\"]  # Cambio de val_loss_values_l2_reg a val_loss_values_dropout\n",
    "\n",
    "# Graficar la p茅rdida de entrenamiento y validaci贸n para cada modelo\n",
    "epochs = range(1, len(loss_values_original) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gr谩fica para el modelo original\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values_original, \"bo\", label=\"P茅rdida de entrenamiento\")\n",
    "plt.plot(epochs, val_loss_values_original, \"b\", label=\"P茅rdida de validaci贸n\")\n",
    "plt.title(\"Modelo Original\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer l铆mites del eje y entre 0 y 1\n",
    "\n",
    "# Gr谩fica para el modelo con Dropout\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss_values_dropout, \"bo\", label=\"P茅rdida de entrenamiento\")  # Cambio de loss_values_l2_reg a loss_values_dropout\n",
    "plt.plot(epochs, val_loss_values_dropout, \"b\", label=\"P茅rdida de validaci贸n\")  # Cambio de val_loss_values_l2_reg a val_loss_values_dropout\n",
    "plt.title(\"Modelo con Dropout\")\n",
    "plt.xlabel(\"pocas\")\n",
    "plt.ylabel(\"P茅rdida\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Establecer l铆mites del eje y entre 0 y 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
